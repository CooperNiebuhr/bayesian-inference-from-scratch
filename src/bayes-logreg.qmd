---
title: "Bayesian Logistic Regression from First Principles"
format:
  html:
    toc: true
    self-contained: true
execute:
  echo: true
  warning: false
  message: false
  cache: true
---

## Overview

This notebook demonstrates a **from-scratch implementation of Bayesian logistic regression** using NumPy, with posterior uncertainty quantified via the Laplace approximation. The implementation includes numerically stable likelihood computations, Newton optimization for MAP estimation, and Monte Carlo integration for predictive inference.

**Key highlights:**
- Complete derivation and implementation of the posterior (prior + likelihood)
- MAP estimation via Newton's method with line search
- Laplace approximation for posterior uncertainty
- Posterior predictive distributions via Monte Carlo sampling
- Validation against PyMC's NUTS sampler

**If you have limited time:** Jump to the **Comparison with PyMC** section to see how the from-scratch implementation matches a state-of-the-art probabilistic programming framework.

**Future directions:** Planned extensions include full posterior sampling via Hamiltonian Monte Carlo and hierarchical Bayesian models with varying intercepts.

## Setup and synthetic data

I generate a well-separated binary classification dataset with two classes of 100 points each. The classes are centered at (-2, 0) and (2, 0) with unit variance. An intercept column is added to enable a bias term in the logistic model.

This controlled setting makes it straightforward to visualize decision boundaries and verify that the implementation behaves correctly. All randomness is seeded for full reproducibility.

```{python}
import numpy as np
#| eval: false

rng = np.random.default_rng(0)
n0 = n1 = 100
X0 = rng.normal(size=(n0,2)) + np.array([-2.0,0.0])
X1 = rng.normal(size=(n1,2)) + np.array([2.0,0.0])
X = np.vstack([X0,X1])
X = np.c_[np.ones(len(X)),X]  # add intercept
y = np.r_[np.zeros(n0), np.ones(n1)]
```

## MAP estimation via Newton's method

The maximum a posteriori (MAP) estimate maximizes the log posterior, which combines a Gaussian prior on the weights with the logistic likelihood:

$$\log p(w \mid D) \propto \log p(D \mid w) + \log p(w)$$

For logistic regression with a Gaussian prior, the log posterior is strictly concave, guaranteeing a unique global maximum. Newton's method exploits this structure by using exact second-order curvature information (the Hessian) to converge rapidly—typically in fewer than 10 iterations.

The implementation includes a backtracking line search to ensure each step increases the log posterior, making the optimizer robust even with poor initializations.

```{python}
from bayes_fp import newton_map_logistic
#| eval: false

w_hat, H_hat, info = newton_map_logistic(X, y, sigma0=0.5, verbose=True)
```

The function returns:
- `w_hat`: the MAP estimate
- `H_hat`: the Hessian at the MAP (needed for the Laplace approximation)
- `info`: a dictionary with convergence diagnostics

## Posterior uncertainty via the Laplace approximation

A point estimate alone doesn't capture parameter uncertainty. The Laplace approximation provides a Gaussian approximation to the posterior by taking a second-order Taylor expansion of the log posterior around the MAP:

$$p(w \mid D) \approx \mathcal{N}(w_{\text{MAP}}, \Sigma)$$

where $\Sigma = -H^{-1}$ and $H$ is the Hessian of the log posterior evaluated at the MAP. This approximation is exact for Gaussian posteriors and works well when the posterior is unimodal and approximately symmetric.

The key advantage is computational efficiency: we reuse the Hessian from optimization rather than running expensive sampling algorithms.

```{python}
from bayes_fp import laplace_covariance
#| eval: false

Sigma = laplace_covariance(H_hat)
```

## Posterior predictive inference

Rather than making predictions with a single weight vector, Bayesian inference propagates uncertainty through to predictions via the **posterior predictive distribution**:

$$p(y_{\text{new}} = 1 \mid x_{\text{new}}, D) = \int \sigma(x_{\text{new}}^T w) \, p(w \mid D) \, dw$$

I approximate this integral via Monte Carlo:
1. Draw weight samples from the Laplace-approximated posterior
2. Compute predictions for each weight sample
3. Average the resulting probabilities

This produces smoother, better-calibrated predictions than using the MAP estimate alone, especially in regions far from the training data where uncertainty is higher.

```{python}
#| eval: false
#| output: false

from bayes_fp import predictive_laplace_mc

probabilities = predictive_laplace_mc(Xnew, w_hat, Sigma, nsamples=5000, seed=1)
```

## Complete example with visualization

Here's the full pipeline in action: data generation, MAP estimation, Laplace approximation, and posterior predictive inference at three test points.

```{python}
import numpy as np
from bayes_fp import newton_map_logistic, laplace_covariance, predictive_laplace_mc

rng = np.random.default_rng(0)
n0 = n1 = 100
X0 = rng.normal(size=(n0,2)) + np.array([-2.0,0.0])
X1 = rng.normal(size=(n1,2)) + np.array([2.0,0.0])
X = np.vstack([X0,X1])
X = np.c_[np.ones(len(X)),X]
y = np.r_[np.zeros(n0), np.ones(n1)]

w_hat, H_hat, info = newton_map_logistic(X, y, sigma0=0.5, verbose=True)
Sigma = laplace_covariance(H_hat)

# Predictions at three test locations
Xnew = np.c_[np.ones(3), np.array([[-3, 0], [0, 0], [3, 0]])]
probabilities = predictive_laplace_mc(Xnew, w_hat, Sigma, nsamples=5000, seed=1)
print("Predicted probabilities at new locations:", probabilities)
print("MAP estimate:", w_hat)
```

The visualization below shows the posterior predictive probability surface across the input space. The decision boundary (0.5 contour) cleanly separates the two classes, and uncertainty increases smoothly in regions far from the training data.

```{python}
#| echo: false
#| cache: true

import matplotlib.pyplot as plt

# --- Fit MAP & Laplace ---
w_hat, H_hat, info = newton_map_logistic(X, y, sigma0=0.5, verbose=False)
Sigma = laplace_covariance(H_hat)

x1_min, x1_max = X[:,1].min()-1.0, X[:,1].max()+1.0
x2_min, x2_max = X[:,2].min()-1.0, X[:,2].max()+1.0
nx, ny = 200, 200
gx = np.linspace(x1_min, x1_max, nx)
gy = np.linspace(x2_min, x2_max, ny)
G1, G2 = np.meshgrid(gx, gy)
G = np.c_[np.ones(G1.size), G1.ravel(), G2.ravel()]

def predictive_grid(Xgrid, batch=20000, nsamples=5000, seed=42):
    probs = np.empty(Xgrid.shape[0], dtype=float)
    start = 0
    while start < Xgrid.shape[0]:
        end = min(start + batch, Xgrid.shape[0])
        probs[start:end] = predictive_laplace_mc(Xgrid[start:end], w_hat, Sigma, nsamples=nsamples, seed=seed+start)
        start = end
    return probs

grid_probs = predictive_grid(G, batch=20000, nsamples=3000, seed=123)
Z = grid_probs.reshape(G1.shape)

# --- Plot: probability heatmap ---
fig, ax = plt.subplots(figsize=(6, 5))

im = ax.imshow(
    Z,
    extent=[x1_min, x1_max, x2_min, x2_max],
    origin="lower",
    aspect="auto",
    alpha=0.6,
)

cs = ax.contour(gx, gy, Z, levels=[0.5], linewidths=2)

ax.scatter(X0[:,0], X0[:,1], s=12, label="class 0")
ax.scatter(X1[:,0], X1[:,1], s=12, label="class 1")

ax.set_xlabel("x1")
ax.set_ylabel("x2")
ax.set_title("Bayesian Logistic Regression: Posterior Predictive Surface")
ax.legend(loc="upper right")

plt.show()

print("Converged:", info.get("converged", None), "in", info.get("num_iter", None), "iterations")
print("MAP:", np.array2string(w_hat, precision=4))
```

## Comparison with PyMC

To validate the implementation, I fit an identical model using PyMC with the No-U-Turn Sampler (NUTS), a state-of-the-art Hamiltonian Monte Carlo algorithm. The model uses the same Gaussian prior ($w \sim \mathcal{N}(0, 0.5^2 I)$) and Bernoulli likelihood.

For this well-behaved problem, the Laplace approximation is expected to work extremely well: the posterior is unimodal, concentrated, and close to Gaussian. Under these conditions, a second-order Taylor expansion captures nearly all the posterior mass, and the approximation $p(w \mid D) \approx \mathcal{N}(w_{\text{MAP}}, \Sigma)$ is highly accurate.

```{python}
#| cache: true
import pymc as pm
import arviz as az

with pm.Model() as pymc_logreg:
    sigma0 = 0.5
    w = pm.Normal("w", mu=0.0, sigma=sigma0, shape=X.shape[1])
    logit_p = pm.math.dot(X, w)
    y_obs = pm.Bernoulli("y_obs", logit_p=logit_p, observed=y)
    
    idata = pm.sample(
        draws=1000,
        tune=1000,
        chains=4,
        target_accept=0.9,
        random_seed=0,
        return_inferencedata=True,
    )

print(az.summary(idata, var_names=["w"], round_to=3))
```

```{python}
#| cache: true
#| echo: false

w_samples = idata.posterior["w"].stack(sample=("chain", "draw")).values
w_mean = w_samples.mean(axis=0)

logits_grid = G @ w_samples
probs_grid = 1.0 / (1.0 + np.exp(-logits_grid))
Z_pymc = probs_grid.mean(axis=1).reshape(nx, ny)
```

The comparison reveals excellent agreement. The PyMC posterior mean is nearly identical to the MAP estimate (differences are negligible relative to coefficient magnitudes), and the decision boundaries overlap almost perfectly. This confirms that:

1. The analytic gradient and Hessian implementations are correct
2. The Newton solver reliably finds the global optimum
3. The Laplace approximation faithfully captures the posterior for this dataset

In more challenging settings—such as weak class separation, strong collinearity, or highly skewed posteriors—the Laplace approximation can break down, and full MCMC sampling becomes necessary. However, for well-behaved problems like this one, the Laplace approach provides an accurate and computationally efficient alternative.

```{python}
#| cache: true
#| echo: false

fig, ax = plt.subplots(figsize=(6, 5))

im = ax.imshow(
    Z,
    extent=[x1_min, x1_max, x2_min, x2_max],
    origin="lower",
    cmap="coolwarm",
    vmin=0.0,
    vmax=1.0,
    aspect="auto",
    alpha=0.4,
)

cs_laplace = ax.contour(gx, gy, Z, levels=[0.5], linewidths=2, linestyles="--")
cs_pymc = ax.contour(gx, gy, Z_pymc, levels=[0.5], linewidths=2)

ax.scatter(X0[:, 0], X0[:, 1], s=12, label="class 0")
ax.scatter(X1[:, 0], X1[:, 1], s=12, label="class 1")

from matplotlib.lines import Line2D

laplace_proxy = Line2D([], [], linestyle="--", color="C0")
pymc_proxy   = Line2D([], [], linestyle="-", color="C0")

handles, labels = ax.get_legend_handles_labels()
handles.extend([laplace_proxy, pymc_proxy])
labels.extend(["Laplace", "PyMC (NUTS)"])

ax.set_xlabel("x1")
ax.set_ylabel("x2")
ax.set_title("Decision Boundary Comparison: Laplace vs. Full MCMC")
ax.legend(handles, labels, loc="upper right")

plt.show()
```

## Summary

This notebook implements Bayesian logistic regression from the ground up, demonstrating proficiency with:

- **Probabilistic modeling**: Deriving and implementing Bayesian posteriors
- **Numerical optimization**: Newton's method with line search for MAP estimation  
- **Uncertainty quantification**: Laplace approximation and posterior predictive inference
- **Scientific computing**: Numerically stable implementations in NumPy
- **Validation**: Comparison against established probabilistic programming tools

The close agreement with PyMC validates the implementation and highlights when computationally efficient approximations can substitute for full sampling algorithms. The modular code structure in `bayes_fp.py` makes it straightforward to extend this work to more complex models and inference methods.